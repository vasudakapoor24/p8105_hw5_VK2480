---
title: "Homework-5"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Library calls
```{r}
library(tidyverse)
```

## Problem 1

```{r, warning=FALSE}
# create dataframe with all file names.

experiment_df = 
  tibble(
    filename = c(list.files("data"))
  )

# create function to read filenames
read_filename <- function(filename) {
  
  filename = str_c("data/", filename)
  file_data = read_csv(filename)
}

# add filedata and tidy dataset
experiment_df = experiment_df %>%
  mutate(
    file_data_list = purrr::map(filename, read_filename)
  ) %>%
  unnest(file_data_list) %>%
  mutate(
    filename = str_replace(filename, ".csv", ""),
    group = str_sub(filename, 1, 3), 
    subject_ID = str_sub(filename, 5)
  )

# mutate data to long version for spaghetti plot

experiment_df = 
  pivot_longer(
    experiment_df, 
    week_1:week_8,
    names_to = "week",
    values_to = "observation",
    names_prefix = "week_"
  )
  
# create spaghetti plot

experiment_plot =
  experiment_df %>%
  ggplot(aes(x = week, y = observation, group = subject_ID, color = subject_ID)) +
  geom_point() +
  geom_path() +
  facet_grid(~group)
experiment_plot
```

## Problem 2

```{r}
#load data and clean names
homicide_df = read_csv("homicide-data.csv") %>%
  janitor::clean_names()
```
Describe raw data: 
This dataset contains `r nrow(homicide_df)` rows and `r ncol(homicide_df)` variables. It provides information about homicides in 50 US cities. Some of the important variables in this dataset include: date of homicide, city, state, and the state/disposition of the homicide case. 

```{r}
# create city_state variable
homicide_df = homicide_df %>%
  mutate(
    city_state = str_c(city, ", ",state)
  )
# Summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”)
total_homicides_df = homicide_df %>%
  group_by(city) %>%
  count() %>%
  rename("total_homicides" = n)

unsolved_homicides_df = homicide_df %>%
  group_by(city) %>%
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>%
  count() %>%
  rename("unsolved_homicides" = n)

homicide_df = left_join(total_homicides_df, unsolved_homicides_df)

# Prop.test for Baltimore

# create function to return tidy version of prop.test results
prop_test_results <- function(prop_x, prop_n) {
  
  prop_test_list = prop.test(prop_x, prop_n)
  prop_test_list = broom::tidy(prop_test_list)
}

balt_prop_test = homicide_df %>%
  filter(city == "Baltimore") %>%
  mutate(
    t_test = prop_test_results(prop_x = unsolved_homicides, prop_n = total_homicides)) %>%
  unnest(cols = t_test) %>%
  select(estimate, conf.low, conf.high) %>%
  mutate(
    CI = str_c(conf.low, ",", conf.high)
  )
balt_prop_test
```

```{r}
# Run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each city.

# create function to return tidy version of prop.test results
prop_test_results <- function(prop_x, prop_n) {
  
  prop_test_list = prop.test(prop_x, prop_n)
  prop_test_list = broom::tidy(prop_test_list)
}

# iterate over dataset to get desired output
homicide_df =
  homicide_df %>%
  mutate(
    prop_test_stats = map2(.x = unsolved_homicides, .y = total_homicides, ~ prop_test_results(prop_x = .x, prop_n = .y))) %>%
  unnest(cols = prop_test_stats) %>%
  select(city, estimate, conf.low, conf.high) %>%
  mutate(
    CI = str_c(conf.low, ", ", conf.high)
  )

# Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

homicide_plot = homicide_df %>%
  ggplot(aes(x = reorder(city, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(legend.position = "none", 
         axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("City")
homicide_plot 
```

## Problem 3

```{r}
# Generate 5000 datasets from the model: x∼Normal[μ,σ].

## step 1: create function to generate required simulation data
sim_mean_pvalue = function(mu = 0, sigma = 5, samp_num = 30) {
  sim_data = tibble(
    x = rnorm(n = samp_num, mean = mu, sd = sigma),
  )
  sim_data %>% 
    summarize(
      mu_hat = mean(x),
      p_value = broom::tidy(t.test(x))[[3]]
    )
}


## step 2: Generate 5000 simulation datasets with mu=0, using the above function

sim0_results_df = 
  expand_grid(
    mu = 0,
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(mu, sim_mean_pvalue)
  ) %>% 
  unnest(estimate_df)
```

```{r}
# Repeat the above for μ={1,2,3,4,5,6}
sim_results_df = 
  expand_grid(
    mu = c(1, 2, 3, 4, 5, 6),
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(mu, sim_mean_pvalue)
  ) %>% 
  unnest(estimate_df)
```

```{r}
# Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.

power_plot_df = sim_results_df %>%
  mutate(conclusion = if_else(p_value > 0.05, "Do not reject", "Reject")) %>%
  group_by(mu, conclusion) %>%
  summarise( n = n()) %>%
  mutate(
    power = n/sum(n)
  ) %>%
  filter(conclusion == "Reject") %>%
  select(mu, power)

power_plot = power_plot_df %>%
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  xlab("True population mean") +
  scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6)) +
  ggtitle("Association between true population means and power of t test")
power_plot 
```
Thus, we can see that power increases with increase in effect size.

```{r}
# Make a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis.
plot2_df1 = sim_results_df %>%
  group_by(mu) %>%
  summarise(
    avg_mu = mean(mu_hat)
  ) %>%
  mutate(
    data = "Null hypothesis was not rejected"
  )

mu_vs_muhat = plot2_df1 %>%
  ggplot(aes(x = mu, y = avg_mu)) +
  geom_point() +
  scale_x_continuous(breaks = seq(0, 6)) +
  scale_y_continuous(breaks = seq(0, 6, by = 0.5)) +
  xlab("True Population Mean") +
  ylab("Average Estimate Mean")

mu_vs_muhat
```


```{r}
# Make a second plot (or overlay on the first) the average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 

plot2_df2 = sim_results_df %>%
  mutate(
    conclusion = if_else(p_value > 0.05, "Do not reject", "Reject")) %>%
  filter(conclusion == "Reject") %>%
  group_by(mu) %>%
  summarise(
    avg_mu = mean(mu_hat)
  ) %>%
  mutate(
    data = "Null hypothesis was rejected"
  )

plot2 = ggplot(data = plot2_df1, aes(x = mu, y = avg_mu, color = data)) + 
  geom_point() + 
  geom_point(data = plot2_df2, shape = 23) +
  scale_x_continuous(breaks = seq(0, 6)) +
  scale_y_continuous(breaks = seq(0, 6, by = 0.5)) +
  xlab("True Population Mean") +
  ylab("Average Estimate Mean")

plot2

```

```{r}
#  Is the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?
```
The sample average of μ̂ across tests for which the null is rejected is not equal to the true value of μ when μ is low, but it gets closer and almost equal to μ as the value of μ increases. 
